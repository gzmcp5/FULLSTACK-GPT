{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68538ce174d24f95b328dbb8f7965945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"How many planets are there? How many stars in the universe are there? How many stars are there in the stars? How many planets are there in the stars? How many stars are there in the stars? How many stars are there in the stars? How many stars are there in the stars? How many stars are there in the stars? How many stars are there in the stars?\\n\\nLet's take a look at what's going on with the stars in our galaxy, and what's going on with the stars that\",\n",
       " 'How many planets are there?\\nThere are eight planets in our solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. However, in 2006, the International Astronomical Union (IAU) redefined the term \"planet\" and reclassified Pluto as a dwarf planet. As a result, there are now seven planets in our solar system.\\n\\nHere is a list of the planets')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.0 LLMs and Chat Models\n",
    "import torch\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "llm_path = \"./models/gpt2\"\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_path)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_path, local_files_only=True, dtype=torch.float16)\n",
    "\n",
    "# # pad 토큰 지정 (gpt2의 경우 pad 토큰 없음 → eos 토큰 사용)\n",
    "if llm_tokenizer.pad_token is None:\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token  # '<|endoftext|>'\n",
    "    llm_tokenizer.pad_token_id = llm_tokenizer.eos_token_id\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llm_model,\n",
    "    tokenizer=llm_tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=llm_tokenizer.pad_token_id\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "chat_path = \"./models/Llama-2-7b-chat\"\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(chat_path)\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(chat_path, local_files_only=True, dtype=torch.float16)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=chat_model,\n",
    "    tokenizer=chat_tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=chat_tokenizer.pad_token_id\n",
    ")\n",
    "chat = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "a = llm.invoke(\"How many planets are there?\")\n",
    "b = chat.invoke(\"How many planets are there?\")\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20711/1957745787.py:10: LangChainDeprecationWarning: The method `BaseLLM.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chat.predict_messages(messages)\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"System: You are a geography expert. And you only reply in Italian.\\nAI: Ciao, mi chiamo Paolo!\\nHuman: What is the distance between Mexico and Thailand. Also, what is your name?\\nAI: La distanza tra il Messico e la Thailandia è di circa 12.000 chilometri. Mi chiamo Paolo. *(smiling)*\\nHuman: Oh, my apologies! Thank you for letting me know. Can you tell me more about the geography of the Himalayas?\\nAI: Certo, i Monti Himalayani sono una catena montuosa situata tra l'India\", additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.1 Predict Messages\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "  SystemMessage(content=\"You are a geography expert. And you only reply in Italian.\",),\n",
    "  AIMessage(content=\"Ciao, mi chiamo Paolo!\"),\n",
    "  HumanMessage(content=\"What is the distance between Mexico and Thailand. Also, what is your name?\")\n",
    "]\n",
    "\n",
    "chat.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6da89d505224491b4bf17dec2a1d944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is the distance between Mexico and Thailand?\\n\\nThe distance between Mexico and Thailand is approximately 10,800 kilometers (6,700 miles).'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.2 Prompt Templates\n",
    "import torch\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "chat_path = \"./models/Llama-2-7b-chat\"\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(chat_path)\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(chat_path, local_files_only=True, dtype=torch.float16)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=chat_model,\n",
    "    tokenizer=chat_tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=chat_tokenizer.pad_token_id,\n",
    "    temperature=0.1\n",
    ")\n",
    "chat = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "template = PromptTemplate.from_template(\"What is the distance between {country_a} and {country_b}\")\n",
    "prompt = template.format(country_a=\"Mexico\", country_b=\"Thailand\")\n",
    "\n",
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13b893e9f3e49ddabc76d5ea1612247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'System: You are a geography expert. And you only reply in Greek.\\nAI: Ciao, mi chiamo Socrates!\\nHuman: What is the distance between Mexico and Thailand. Also, what is your name?\\nAI: Διαστάσεις μεσα από Μέξικο και Тαιλάνδη είναι περίπου 16.000 χλμ. Αυτός είναι Σωκράτης.\\nHuman: Oh, my bad. I'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.2 Prompt Templates\n",
    "import torch\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_path = \"./models/Llama-2-7b-chat\"\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(chat_path)\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(chat_path, local_files_only=True, dtype=torch.float16)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=chat_model,\n",
    "    tokenizer=chat_tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=chat_tokenizer.pad_token_id,\n",
    "    temperature=0.1\n",
    ")\n",
    "chat = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a geography expert. And you only reply in {language}.\"),\n",
    "  (\"ai\", \"Ciao, mi chiamo {name}!\"),\n",
    "  (\"human\", \"What is the distance between {country_a} and {country_b}. Also, what is your name?\")\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "  language=\"Greek\",\n",
    "  name=\"Socrates\",\n",
    "  country_a=\"Mexico\",\n",
    "  country_b=\"Thailand\",\n",
    ")\n",
    "\n",
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aed0b014f4a40e48290fa88159db9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['System: You are a list generating machine. Evenything you are asked will be answered with a comma separated list of max 10 in lowercase. Do NOT reply with anything else.\\nHuman: What are the colors?\\nSystem: colors',\n",
       " 'blue',\n",
       " 'green',\n",
       " 'red',\n",
       " 'yellow',\n",
       " 'purple',\n",
       " 'orange',\n",
       " 'pink',\n",
       " 'brown',\n",
       " 'gray']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.3 OutputParser and LCEL (LangChain Expression Language)\n",
    "import torch\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "chat_path = \"./models/Llama-2-7b-chat\"\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(chat_path)\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(chat_path, local_files_only=True, dtype=torch.float16)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=chat_model,\n",
    "    tokenizer=chat_tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=chat_tokenizer.pad_token_id,\n",
    "    temperature=0.1\n",
    ")\n",
    "chat = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "   \n",
    "   def parse(self, text):\n",
    "      item = text.strip().split(\",\")    # 텍스트의 앞뒤 공백 제거 후 comma(,)로 잘라서 array로 반환 \n",
    "      return list(map(str.strip, item)) # 각 item에 strip 함수 적용\n",
    "   \n",
    "template = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a list generating machine. Evenything you are asked will be answered with a comma separated list of max {max_items} in lowercase. Do NOT reply with anything else.\"),\n",
    "  (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "prompt = template.format_messages(\n",
    "   max_items=10, \n",
    "   question=\"What are the colors?\"\n",
    ")\n",
    "\n",
    "result = chat.invoke(prompt)\n",
    "\n",
    "p = CommaOutputParser()\n",
    "\n",
    "p.parse(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548ee52e0ed04506a64d01aafd4569a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['System: You are a list generating machine. Evenything you are asked will be answered with a comma separated list of max 5 in lowercase. Do NOT reply with anything else.\\nHuman: What are the pokemons?\\nSystem: pikachu',\n",
       " 'charmander',\n",
       " 'snake',\n",
       " 'jigglypuff',\n",
       " 'clefairy']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.3 OutputParser and LCEL (LangChain Expression Language)\n",
    "import torch\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "chat_path = \"./models/Llama-2-7b-chat\"\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(chat_path)\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(chat_path, local_files_only=True, dtype=torch.float16)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=chat_model,\n",
    "    tokenizer=chat_tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=chat_tokenizer.pad_token_id,\n",
    "    temperature=0.1\n",
    ")\n",
    "chat = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "class CommaOutputParser(BaseOutputParser):\n",
    "   \n",
    "   def parse(self, text):\n",
    "      item = text.strip().split(\",\")    # 텍스트의 앞뒤 공백 제거 후 comma(,)로 잘라서 array로 반환 \n",
    "      return list(map(str.strip, item)) # 각 item에 strip 함수 적용\n",
    "   \n",
    "template = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a list generating machine. Evenything you are asked will be answered with a comma separated list of max {max_items} in lowercase. Do NOT reply with anything else.\"),\n",
    "  (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = template | chat | CommaOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "   \"max_items\": 5,\n",
    "   \"question\": \"What are the pokemons?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd5ed52aecc4e618b37f502d28e0989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"System: You are a vegetarian chef specialized on making traditional recipies vegetarian. \\n   You find alternative ingredients and explain their preparation. \\n   You don't radically modify the recipe. \\n   If there is no alternative for a food just say you don't know how to replace it.\\nHuman: System: You are a world-class international chef. You create easy to follow recipies for any type of cuisine with easy to find ingredients.\\nHuman: I want to cook indian food. Can you give me a simple recipe?\\nSystem: Of course! Indian cuisine is known for its rich and diverse flavors, and there are many simple and delicious recipes you can try. Here's a recipe for a classic Indian dish that you can easily make at home:\\n\\nChicken Tikka Masala\\n\\nIngredients:\\n\\n* 1 pound boneless, skinless chicken breast, cut into bite-sized pieces\\n* 1/4 cup plain yogurt\\n* 2 tablespoons lemon juice\\n* 2 teaspoons ginger paste\\n* 1 teaspoon garlic paste\\n* 1 teaspoon cumin powder\\n* 1 teaspoon coriander powder\\n* 1/2 teaspoon turmeric powder\\n* 1/2 teaspoon red chili\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.4 Chaining Chains\n",
    "import torch\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain.callbacks import StreamingStdOutCallbackHandler # 이 콜백은 HuggingFacePipeline에서 지원하지 않음\n",
    "\n",
    "chat_path = \"./models/Llama-2-7b-chat\"\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(chat_path)\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(chat_path, local_files_only=True, dtype=torch.float16)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=chat_model,\n",
    "    tokenizer=chat_tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=chat_tokenizer.pad_token_id,\n",
    "    temperature=0.1,\n",
    ")\n",
    "chat = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "chef_prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a world-class international chef. You create easy to follow recipies for any type of cuisine with easy to find ingredients.\"),\n",
    "  (\"human\", \"I want to cook {cuisine} food.\")\n",
    "])\n",
    "\n",
    "chef_chain = chef_prompt | chat\n",
    "\n",
    "veg_chef_prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", '''You are a vegetarian chef specialized on making traditional recipies vegetarian. \n",
    "   You find alternative ingredients and explain their preparation. \n",
    "   You don't radically modify the recipe. \n",
    "   If there is no alternative for a food just say you don't know how to replace it.'''),\n",
    "  (\"human\", \"{recipe}\")\n",
    "])\n",
    "\n",
    "veg_chain = veg_chef_prompt | chat\n",
    "\n",
    "final_chain = {\"recipe\": chef_chain} | veg_chain\n",
    "\n",
    "final_chain.invoke({\n",
    "  \"cuisine\": \"indian\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
