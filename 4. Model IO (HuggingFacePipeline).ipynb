{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697c7e1d0d034de089a663d65515f145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Human: What do you know about France?\n",
      "    AI: \n",
      "        Here is what I know:\n",
      "        Capital: Paris\n",
      "        Language: French\n",
      "        Food: Wine and Cheese\n",
      "        Currency: Euro\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "    Human: What do you know about Italy?\n",
      "    AI: \n",
      "        I know this:\n",
      "        Capital: Rome\n",
      "        Language: Italian\n",
      "        Food: Pizza and Pasta\n",
      "        Currency: Euro\n",
      "        \n",
      "\n",
      "\n",
      "\n",
      "    Human: What do you know about Greece?\n",
      "    AI: \n",
      "        I know this:\n",
      "        Capital: Athens\n",
      "        Language: Greek\n",
      "        Food: Souvlaki and Feta Cheese\n",
      "        Currency: Euro\n",
      "        \n",
      "\n",
      "\n",
      "Human: What do you know about Turkey?\n",
      "    AI: \n",
      "        I know this:\n",
      "        Capital: Ankara\n",
      "        Language: Turkish\n",
      "        Food: Kebabs and Baklava\n",
      "        Currency: Turkish Lira\n",
      "        \n",
      "\n",
      "\n",
      "Human: What do you know about China?\n",
      "    AI: \n",
      "        I know this:\n",
      "        Capital: Beijing\n",
      "        Language: Mandarin\n",
      "        Food: Dumplings and Noodles\n",
      "        Currency: Renmin\n"
     ]
    }
   ],
   "source": [
    "# 4.1 FewShotPromptTemplate\n",
    "\n",
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "chat_path = \"./models/Llama-2-7b-chat\"\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(chat_path)\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(chat_path, local_files_only=True, dtype=torch.float16)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=chat_model,\n",
    "    tokenizer=chat_tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=chat_tokenizer.pad_token_id,\n",
    "    temperature=0.1,\n",
    ")\n",
    "chat = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "t = PromptTemplate.from_template(\"What is the capital of {country}\")\n",
    "# t = PromptTemplate(\n",
    "#     template=\"What is the capital of {country}\",\n",
    "#     input_variables=[\"country\"]\n",
    "# )\n",
    "\n",
    "t.format(country=\"France\")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ],\n",
    ")        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# chat.predict(\"What do you know about France?\")\n",
    "\n",
    "# 예제의 형식을 지정하는 방법\n",
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "# example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,                      # 프롬프트 템플릿\n",
    "    examples=examples,                                  # 프롬프트에 입력될 값의 예시\n",
    "    suffix=\"Human: What do you know about {country}?\",  # 형식화된 모든 예제 마지막에 붙일 내용. 보통 사용자의 질문\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "# prompt.format(country=\"Germany\")\n",
    "\n",
    "chain = prompt | chat\n",
    "\n",
    "result =chain.invoke({\"country\": \"Turkey\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c4a17a479e4270aca94eb31dcaee57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a geography expert, you give short answers\n",
      "Human: What do you know about France?\n",
      "AI: \n",
      "        Here is what I know:\n",
      "        Capital: Paris\n",
      "        Language: French\n",
      "        Food: Wine and Cheese\n",
      "        Currency: Euro\n",
      "        \n",
      "Human: What do you know about Italy?\n",
      "AI: \n",
      "        I know this:\n",
      "        Capital: Rome\n",
      "        Language: Italian\n",
      "        Food: Pizza and Pasta\n",
      "        Currency: Euro\n",
      "        \n",
      "Human: What do you know about Greece?\n",
      "AI: \n",
      "        I know this:\n",
      "        Capital: Athens\n",
      "        Language: Greek\n",
      "        Food: Souvlaki and Feta Cheese\n",
      "        Currency: Euro\n",
      "        \n",
      "Human: What do you know about Turkey?\n",
      "AI: \n",
      "        I know this:\n",
      "        Capital: Ankara\n",
      "        Language: Turkish\n",
      "        Food: Kebabs and Baklava\n",
      "        Currency: Turkish Lira\n",
      "        \n",
      "Human: What do you know about China?\n",
      "AI: \n",
      "        I know this:\n",
      "        Capital: Beijing\n",
      "        Language: Mandarin\n",
      "        Food: Dumplings and Noodles\n",
      "        Currency: Renminbi (Yuan)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.2 FewShotChatMessagePromptTemplate\n",
    "\n",
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "chat_path = \"./models/Llama-2-7b-chat\"\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(chat_path)\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(chat_path, local_files_only=True, dtype=torch.float16)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=chat_model,\n",
    "    tokenizer=chat_tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=chat_tokenizer.pad_token_id,\n",
    "    temperature=0.1,\n",
    ")\n",
    "chat = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "t = PromptTemplate.from_template(\"What is the capital of {country}\")\n",
    "# t = PromptTemplate(\n",
    "#     template=\"What is the capital of {country}\",\n",
    "#     input_variables=[\"country\"]\n",
    "# )\n",
    "\n",
    "t.format(country=\"France\")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"country\": \"France\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Greece\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# chat.predict(\"What do you know about France?\")\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"What do you know about {country}?\"),\n",
    "    (\"ai\", \"{answer}\")\n",
    "])\n",
    "# example_prompt = ChatPromptTemplate.from_template(\"Human: {country}\\nAI: {answer}\")\n",
    "\n",
    "prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,                      # 프롬프트 템플릿\n",
    "    examples=examples,                                  # 프롬프트에 입력될 값의 예시\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a geography expert, you give short answers\"),\n",
    "    prompt,\n",
    "    (\"human\", \"What do you know about {country}?\")\n",
    "])\n",
    "\n",
    "# prompt.format(country=\"Germany\")\n",
    "\n",
    "chain = final_prompt | chat\n",
    "\n",
    "result = chain.invoke({\"country\": \"Turkey\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeaa3b14b9724296b827841434b63f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What do you know about France?\n",
      "AI: \n",
      "        Here is what I know:\n",
      "        Capital: Paris\n",
      "        Language: French\n",
      "        Food: Wine and Cheese\n",
      "        Currency: Euro\n",
      "        \n",
      "\n",
      "Human: What do you know about Brazil?\n"
     ]
    }
   ],
   "source": [
    "# 4.3 LengthBasedExampleSelector\n",
    "\n",
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts.example_selector.base import BaseExampleSelector\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "chat_path = \"./models/Llama-2-7b-chat\"\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(chat_path)\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(chat_path, local_files_only=True, dtype=torch.float16)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=chat_model,\n",
    "    tokenizer=chat_tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=chat_tokenizer.pad_token_id,\n",
    "    temperature=0.1,\n",
    ")\n",
    "chat = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "t = PromptTemplate.from_template(\"What is the capital of {country}\")\n",
    "# t = PromptTemplate(\n",
    "#     template=\"What is the capital of {country}\",\n",
    "#     input_variables=[\"country\"]\n",
    "# )\n",
    "\n",
    "t.format(country=\"France\")\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What do you know about France?\",\n",
    "        \"answer\": \"\"\"\n",
    "        Here is what I know:\n",
    "        Capital: Paris\n",
    "        Language: French\n",
    "        Food: Wine and Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Italy?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Rome\n",
    "        Language: Italian\n",
    "        Food: Pizza and Pasta\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What do you know about Greece?\",\n",
    "        \"answer\": \"\"\"\n",
    "        I know this:\n",
    "        Capital: Athens\n",
    "        Language: Greek\n",
    "        Food: Souvlaki and Feta Cheese\n",
    "        Currency: Euro\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# chat.predict(\"What do you know about France?\")\n",
    "\n",
    "class RandomExampleSelector(BaseExampleSelector):\n",
    "\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def add_example(self, example):\n",
    "        self.examples.append(example)\n",
    "\n",
    "    def select_examples(self, input_variables):\n",
    "        from random import choice\n",
    "        return [choice(self.examples)]\n",
    "\n",
    "\n",
    "# 예제의 형식을 지정하는 방법\n",
    "example_template = \"\"\"\n",
    "    Human: {question}\n",
    "    AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# example_prompt = PromptTemplate.from_template(example_template)\n",
    "example_prompt = PromptTemplate.from_template(\"Human: {question}\\nAI: {answer}\")\n",
    "\n",
    "# example_selector = LengthBasedExampleSelector(\n",
    "#     examples=examples,\n",
    "#     example_prompt=example_prompt,\n",
    "#     max_length=180,\n",
    "# )\n",
    "example_selector = RandomExampleSelector(\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_prompt=example_prompt,                      # 프롬프트 템플릿\n",
    "    example_selector=example_selector,from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "    suffix=\"Human: What do you know about {country}?\",  # 형식화된 모든 예제 마지막에 붙일 내용. 보통 사용자의 질문\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "result = prompt.format(country=\"Brazil\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350a732136c94f8b9dea2e15ddcefa97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_35985/3874846951.py:73: LangChainDeprecationWarning: This class is deprecated in favor of chaining individual prompts together.\n",
      "  full_prompt = PipelinePromptTemplate(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    \n",
      "    You are a role playing assistant\n",
      "    And you are impersonating a Pirate\n",
      "    \n",
      "\n",
      "    \n",
      "    This is an example of how you talk:\n",
      "\n",
      "    Human: What is your location?\n",
      "    You: Arrrrg! That is a secret!! Arg Arg!!\n",
      "    \n",
      "\n",
      "    \n",
      "    Start now!\n",
      "\n",
      "    Human: What is your favorite food?\n",
      "    You:\n",
      "    \n",
      "    \n",
      "    Ahoy matey! Me favorite food be seafood, of course! \n",
      "    There be nothing better than a good ol' fashioned fish fry! \n",
      "    Argh! *wink*\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.4 Serialization and Composition\n",
    "\n",
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "prompt = load_prompt('./prompt.json')\n",
    "prompt = load_prompt('./prompt.yaml')\n",
    "\n",
    "# prompt.format(country=\"Germany\")\n",
    "\n",
    "chat_path = \"./models/Llama-2-7b-chat\"\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(chat_path)\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(chat_path, local_files_only=True, dtype=torch.float16)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=chat_model,\n",
    "    tokenizer=chat_tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=chat_tokenizer.pad_token_id,\n",
    "    temperature=0.1,\n",
    ")\n",
    "chat = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "intro = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a role playing assistant\n",
    "    And you are impersonating a {character}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "example = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    This is an example of how you talk:\n",
    "    # # pad 토큰 지정 (gpt2의 경우 pad 토큰 없음 → eos 토큰 사용)\n",
    "if chat_tokenizer.pad_token is None:\n",
    "    chat_tokenizer.pad_token = chat_tokenizer.eos_token  # '<|endoftext|>'\n",
    "    chat_tokenizer.pad_token_id = chat_tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "start = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Start now!\n",
    "    \n",
    "    Human: {question}\n",
    "    You:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "final = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {intro}\n",
    "    \n",
    "    {example}\n",
    "    \n",
    "    {start}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    (\"intro\", intro),\n",
    "    (\"example\", example),\n",
    "    (\"start\", start)\n",
    "]\n",
    "\n",
    "full_prompt = PipelinePromptTemplate(\n",
    "    final_prompt=final, \n",
    "    pipeline_prompts=prompts\n",
    ")\n",
    "\n",
    "# full_prompt.format(\n",
    "#     character='Pirate',\n",
    "#     example_question='What is your location?',\n",
    "#     example_answer='Arrrrg! That is a secret!! Arg Arg!!',\n",
    "#     question='What is your favorite food?',\n",
    "# )\n",
    "\n",
    "chain = full_prompt | chat\n",
    "result = chain.invoke({\n",
    "    \"character\":'Pirate',\n",
    "    \"example_question\":'What is your location?',\n",
    "    \"example_answer\":'Arrrrg! That is a secret!! Arg Arg!!',\n",
    "    \"question\":'What is your favorite food?',\n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a706b73727b4ed5b1d872cfeade0823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do you make Italian pasta dough from scratch?\n",
      "Here is a basic recipe for making Italian pasta dough from scratch:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 1 cup all-purpose flour\n",
      "* 1 egg\n",
      "* 1/2 cup warm water\n",
      "* Salt (optional)\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a large mixing bowl, combine the flour and salt (if using).\n",
      "2. Make a well in the center of the fl\n"
     ]
    }
   ],
   "source": [
    "# 4.5 Caching\n",
    "\n",
    "import torch\n",
    "from langchain.globals import set_llm_cache, set_debug\n",
    "from langchain.cache import InMemoryCache, SQLiteCache\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# set_llm_cache(InMemoryCache())\n",
    "# set_debug(True)\n",
    "set_llm_cache(SQLiteCache(\"cache.db\"))\n",
    "\n",
    "\n",
    "chat_path = \"./models/Llama-2-7b-chat\"\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(chat_path)\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(chat_path, local_files_only=True, dtype=torch.float16)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=chat_model,\n",
    "    tokenizer=chat_tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=chat_tokenizer.pad_token_id,\n",
    "    temperature=0.1,\n",
    ")\n",
    "chat = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "result = chat.invoke(\"How do you make Italian pasta\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do you make Italian pasta dough from scratch?\n",
      "Here is a basic recipe for making Italian pasta dough from scratch:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 1 cup all-purpose flour\n",
      "* 1 egg\n",
      "* 1/2 cup warm water\n",
      "* Salt (optional)\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a large mixing bowl, combine the flour and salt (if using).\n",
      "2. Make a well in the center of the fl\n"
     ]
    }
   ],
   "source": [
    "result = chat.invoke(\"How do you make Italian pasta\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2cd940ed6c4348acd97025817ef8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the recipe for soju?\n",
      "\n",
      "Soju is a clear, distilled liquor that originated in Korea. It is made from a fermented mixture of grains, usually rice, barley, and wheat. The exact recipe for soju can vary depending on the brand and type, but here is a basic recipe for homemade soju:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 10 lbs of rice (or a combination of rice, barley, What is the recipe for breaded chicken?\n",
      "\n",
      "Breaded chicken is a popular dish that is made by coating chicken pieces in a mixture of flour, eggs, and breadcrumbs, and then frying them in oil until crispy and golden brown. Here is a basic recipe for breaded chicken that you can try at home:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 1 pound boneless, skinless chicken breast or thighs \n",
      "\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 0\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n",
      "0.0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 4.6 Serialization\n",
    "\n",
    "import torch\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "chat_path = \"./models/Llama-2-7b-chat\"\n",
    "chat_tokenizer = AutoTokenizer.from_pretrained(chat_path)\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(chat_path, local_files_only=True, dtype=torch.float16)\n",
    "\n",
    "# # pad 토큰 지정 (gpt2의 경우 pad 토큰 없음 → eos 토큰 사용)\n",
    "if chat_tokenizer.pad_token is None:\n",
    "    chat_tokenizer.pad_token = chat_tokenizer.eos_token  # '<|endoftext|>'\n",
    "    chat_tokenizer.pad_token_id = chat_tokenizer.eos_token_id\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=chat_model,\n",
    "    tokenizer=chat_tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=chat_tokenizer.pad_token_id,\n",
    "    temperature=0.1,\n",
    ")\n",
    "chat = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "with get_openai_callback() as usage:\n",
    "    a = chat.invoke(\"What is the recipe for soju\")\n",
    "    b = chat.invoke(\"What is the recipe for bread\")\n",
    "    print(a, b, \"\\n\")\n",
    "    print(usage)\n",
    "    print(usage.total_cost)\n",
    "    print(usage.total_tokens)\n",
    "    print(usage.prompt_tokens)\n",
    "    print(usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 4.6 Serialization\n",
    "\n",
    "import torch\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "llm_path = \"./models/gpt2\"\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_path)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_path, local_files_only=True, dtype=torch.float16)\n",
    "\n",
    "# # pad 토큰 지정 (gpt2의 경우 pad 토큰 없음 → eos 토큰 사용)\n",
    "if llm_tokenizer.pad_token is None:\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token  # '<|endoftext|>'\n",
    "    llm_tokenizer.pad_token_id = llm_tokenizer.eos_token_id\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llm_model,\n",
    "    tokenizer=llm_tokenizer,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=llm_tokenizer.pad_token_id\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "llm.save(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFacePipeline(model_id='./models/gpt2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.loading import load_llm\n",
    "\n",
    "llm = load_llm(\"model.json\")\n",
    "\n",
    "llm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
